---
layout: post
title: "Samza Job 101"
description: ""
category: 
tags: [samza, yarn, kafka]
---

### How to set up dev environment? ###

Easiest way is to put yarn, kafka, and samza inside a single docker container. You can then share the docker image with the team. Although you can put yarn and kafka in separate docker containers and compose them, this sometimes has port/host name resolution problem


### How To make a samsa job? ###

You need to make: 

* a samza package, which is a tarball 
* a samza job configuration file.


### What is a samza package? ###

Just a tarball with the following structure (unzipped)

	 bin/run-am.sh
	 bin/run-class.sh
	 bin/run-job.sh
	 bin/run-container.sh
	 lib/ 

All jars, including samza ones, are in lib/	

The .sh files are from dependency 

	 groupID: org.apahce.samza 
	 artifactID: samza-shell
	 classifier: dist
	 type: tgz   

### What is in the JARs? ###

The class represents your job processing logic. All your stream processing logic should be in a class that implement org.apache.samza.task.StreamTask interface, and more specifically, its process() method. 

Note that all jars from samza will be in lib as well. This is part of their design decision to isolate dependency


### How to implement process() method? ###

* read incoming messages from envelope
* use collector to send output message wrapped in OutgoingMessageEnveloope. NOTE: do NOT use collector outside process() 
* coordinator for advanced use


###How to make configuration file? ###

Essentially a  .properties file for java programs

At minimum, you need to define:
* job.factory.class
* job.name
* task.class
* task.inputs

Detailed explanation will be in another post

###How to build and deploy package? ###
* Create a configuration .properties file for your new task class
* sudo mvn package
* put the generated tarball to a location the worker process/box can access. When developing locally, a file path is generally enough.

### How to run the package? ###

From shell

```
$PACKAGE_PATH/bin/run-job.sh --config-factory=samza.config.factories.PropertiesConfigFactory --config-path=$CONFIG_FILE_PATH
```

You can run job from code as well. This will be explained in another post

### How to know if my job is running correctly? ###
* YARN web UI lists running, waiting, killed jobs. web UI port default is 8088
* check the $YARN_BIN_PATH/logs/usrlog. 

### How to kill my running job? ###
Just kill it via yarn, e.g., via yarn REST API
```
curl -X PUT -d '{"state":"KILLED"}' -i http://$YARN_HOST:8088/ws/v1/cluster/apps/$APP_ID/state --header "Content-Type:application/json"
```


Samza task container: a quick overview
----------
Each container is single-threaded. This means container will multiplex messages
and functions to task instances inside the container

### What in the run() loop ? ###

1. Choose one envelope from internal message chooser

2. Update message chooser with an envelope from the supplied stream partition 

3. Take the head envelope off the message queue for that stream partition

4. Update the message chooser

5. Based on the stream partition, get a task instance

6. Task instance will process this envelope,i.e., call process() of your StreamTask

7. Window function operation

8. Samza commit logic 


### How does container get messages to process? ###

Container refreshes message queues periodically, or if there is no message to process is empty. Notice the check is done inside the same thread, since task instance itself is single-threaded

Every time container refreshes, it will try to poll new data from underlying consumer. 

For Kafka, the underlying consumer is implemented in an asynchronous way, i.e., Kafka consumer uses BlockingQueue as buffer, which will answer poll requeset from task container. Kafka consumer also spawns one thread for each stream partition as worker threads to fill the buffer
