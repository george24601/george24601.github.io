---
layout: post
title: "System design: Chat systems" 
description: ""
category: 
tags: [interview,arch]
---

### Requirements

* Multiple clients for a single user is allowed, and all messages should be synced between the clients
* Messages are displayed in the same order on all clients, senders, and receivers.
  * My interpretation is that message arrival can be delayed but not out of order, i.e., if message order is 1, 2, it is OK to delay the delivery of 2 until 1 is delivered, but it is not ok to display 2 on client, and then insert 1 in front of 2.
* Web scale
* Use WebSocket to deliver messages
* User Authentication needed
* Can ignore friend list feature for now

### Capacity estimation

* 1B user clients. Each sends 1 message every 2 min in 12 hours = 1B * 1 * 30 * 12  = 360B messsages
* Each message 1kb = 360 TB storage per day 
* Peak write traffic = 4 times of average about 35 mil writes per sec 
* Because the read is powered by websocket, the read request will be higher by write but not much higher. 
* Each websocket server maintains 100k connections, so we need 10k node
* Each DB node handles 10k request per sec peak, so need a 350 node cluster


### High level components breakdown

Excluding common services, we need

* User authenticaiton and authorization module (UAA)
  * SSO such as open id
  * distributed authentication such as JWT token
* User domain service that stores detailed user info
* Message domain service
* Dialog domain service, i.e, read-only API service
  * Seperation of the message domain and dialog domain is needed, e.g., message itself is subject to extension.
* Notification service to host websocket connections to clients 

### Message service

* Maintains the mapping (msg_id) -> (sender, receiver, content, status, create_at, updated_at). msg_id is a snowflake id which is roughly sorted by the processing time
* Status is to mark if the message has been delivered to the dialog service. 
  * Alternatively can use a message queue to deliver msg to the dialog service. It is a trade off between sync and async and either could work.
  * However, in either way we need 2PC/TCC support between the message domain and dialog domain because we need to maintain a consistent order across all clients.
* Only after the we deliver to the dialog service that we return to the client the message has been delivered. Again, if we don't have to maintain a consistent order, we can return to client the moment it is persisted to message db, and do the delivery async

#### 2PC/TCC for between message and dialog

can dive deep on this later
 
### Dialog service

Maintains the following mappings 
  * (receiver_id, message_created_at, sender_id) -> (message_id, status) 
  * (receiver_id)  -> (last_polled_at) 
  * Underlying DB choice: Cassandra or sharded mysql. It doesn't matter much. Either way we will use receiver_id as the partition key, and the key of the mapping as the sort key within the partition 
* Upon persisting message from the message service, dialog service send a notification via notification service to client that new messages are available
  * Client will poll from dialog service, upon successful delivery, we will update the last_polled_at checkpoint for this receiver
  * Can also push directly to the client as part of notification instead of notifying client to poll. Most likely a hybrid to be reactive
* Delivered messaged can be purged from the DB, since we have message service as source of truth

### Cilent

* In memory maintains received message ids and their created_at so that client can do local dedup and reordering (which should not be needed in this case)
* Client does not send out another message until the previous one has been acked by the server side. 
  * Inputing message is a low throughput activity so such delay is OK. 
  * If we really want to support concurrent send, then we have to maintain a sequence number for each dialog, which further complicates the design
* Note we can't rely on client time because of the consist, in order requirement - the processing time in the message service is the source of truth

### Notification service

* This is the layer that maintains the long-living client connections
* Need a centralized service discovery router to route the incoming notifiction request to the real server
 * 1B client * 1KB each entry = 1T storage
 * Simple ID based sharding + 10 nodes is enough to host the metadata
